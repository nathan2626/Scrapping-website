import json
import requests
from bs4 import BeautifulSoup

# URL cible
with open('url.txt', 'r') as file:
    URL = file.read().strip()

# Headers pour √©viter d'√™tre bloqu√© par le site
headers = {'User-Agent': 'Mozilla/5.0'}

# Effectuer la requ√™te HTTP
response = requests.get(URL, headers=headers)

if response.status_code == 200:
    soup = BeautifulSoup(response.text, "html.parser")
    
    # Trouver les blocs contenant les soci√©t√©s savantes
    societies = soup.find_all("article", class_="article-list-item")

    data = []
    
    for society in societies:
        details = {}

        # R√©cup√©rer les informations demand√©es
        details["Organisation"] = society.find("h2").text.strip() if society.find("h2") else "N/A"
        details["Abr√©viation"] = society.find("h3").text.strip() if society.find("h3") else "N/A"
        
        # R√©cup√©rer sp√©cialit√© et type
        spans = society.find_all("span", class_="info-label")
        for span in spans:
            label = span.text.strip().lower()
            value = span.find_next_sibling("span").text.strip() if span.find_next_sibling("span") else "N/A"
            if "sp√©cialit√©" in label:
                details["Sp√©cialit√©"] = value
            elif "type" in label:
                details["Type"] = value
        
        # R√©cup√©rer les liens utiles
        links = society.find_all("a", href=True)
        for link in links:
            href = link["href"]
            text = link.text.strip().lower()
            if "actualit√©s" in text:
                details["Actualit√©s"] = href
            elif "rss" in text:
                details["RSS"] = href
            elif "publications ouvertes" in text:
                details["Publications ouvertes"] = href
            elif "publications rss" in text:
                details["Publications RSS"] = href
            elif "social" in text:
                details["Social"] = href
        
        # Ajouter un contr√¥le pour les champs manquants
        details.setdefault("Actualit√©s", "N/A")
        details.setdefault("RSS", "N/A")
        details.setdefault("Publications ouvertes", "N/A")
        details.setdefault("Publications RSS", "N/A")
        details.setdefault("Social", "N/A")

        data.append(details)

    # üìÇ Sauvegarde dans un fichier JSON
    output_file = "1_scraping_societes/societes_savantes.json"
    with open(output_file, "w", encoding="utf-8") as file:
        json.dump(data, file, indent=4, ensure_ascii=False)

    print(f"\n‚úÖ Donn√©es enregistr√©es dans : {output_file}")

else:
    print(f"‚ùå √âchec de r√©cup√©ration de la page, code HTTP: {response.status_code}")
