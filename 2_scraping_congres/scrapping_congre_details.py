import json
import requests
from bs4 import BeautifulSoup
import os

# üìÇ Chemins des fichiers mis √† jour
INPUT_FILE = "3_filtrage_congres/liste_congr√®s_reformat.json"
OUTPUT_FILE = "2_scraping_congres/congres_bruts.json"
LOG_FILE = "2_scraping_congres/scraping_errors.log"

# V√©rifier si le fichier d'entr√©e existe
if not os.path.exists(INPUT_FILE):
    print(f"‚ùå Erreur : Le fichier {INPUT_FILE} n'existe pas.")
    exit(1)

# Charger la liste de liens depuis le fichier JSON
with open(INPUT_FILE, "r", encoding="utf-8") as file:
    urls = json.load(file)  # Assurez-vous que le fichier contient une liste de liens

if not urls:
    print("‚ùå Aucune URL trouv√©e dans le fichier d'entr√©e.")
    exit(1)

# S√©lecteurs CSS pour extraire les donn√©es
SELECTORS = {
    "lieu": "div:nth-of-type(2) > p:nth-of-type(1) > a",
    "pays": "div:nth-of-type(2) > p:nth-of-type(2) > a > span",
    "langue": "div:nth-of-type(2) > p:nth-of-type(3) > span",
    "date_debut": "div:nth-of-type(2) > p:nth-of-type(4) > time > a",
    "date_fin": "div:nth-of-type(2) > p:nth-of-type(5) > time",
    "lien": "div:nth-of-type(2) > p:nth-of-type(6) > a",
    "description": "div:nth-of-type(3)",
    "specialites": "div:nth-of-type(2) > p:nth-of-type(7) > a"
}

# Headers pour √©viter d'√™tre bloqu√© par certains sites
HEADERS = {'User-Agent': 'Mozilla/5.0'}

# Fonction pour scraper une page donn√©e
def scrape_page(url):
    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        response.raise_for_status()  # V√©rifier si la requ√™te a r√©ussi
        soup = BeautifulSoup(response.content, "html.parser")

        # Extraire les donn√©es avec BeautifulSoup
        data = {}
        for key, selector in SELECTORS.items():
            elements = soup.select(selector)
            if key == "specialites":
                data[key] = [el.get_text(strip=True) for el in elements] or ["Non disponible"]
            elif key == "description":
                data[key] = elements[0].get_text(strip=True) if elements else "Non disponible"
            else:
                data[key] = elements[0].get_text(strip=True) if elements else "Non disponible"

        data["lien_source"] = url  # Ajouter le lien source
        return data

    except requests.RequestException as e:
        # Enregistrer les erreurs dans un fichier
        with open(LOG_FILE, "a", encoding="utf-8") as log_file:
            log_file.write(f"‚ö†Ô∏è Erreur lors de l'acc√®s √† {url} : {e}\n")
        print(f"‚ö†Ô∏è Erreur lors de l'acc√®s √† {url} : {e}")
        return None

# Scraper toutes les pages et stocker les r√©sultats
resultats = []
total_urls = len(urls)

print(f"üîç D√©but du scraping de {total_urls} congr√®s...")

for index, url in enumerate(urls, start=1):
    print(f"‚û°Ô∏è [{index}/{total_urls}] Scraping de {url}...")
    data = scrape_page(url)
    if data:
        resultats.append(data)

# Sauvegarder les r√©sultats dans un fichier JSON
with open(OUTPUT_FILE, "w", encoding="utf-8") as file:
    json.dump(resultats, file, indent=4, ensure_ascii=False)

print(f"\n‚úÖ Scraping termin√©. R√©sultats sauvegard√©s dans '{OUTPUT_FILE}'.")
if os.path.exists(LOG_FILE):
    print(f"‚ö†Ô∏è Des erreurs ont √©t√© enregistr√©es dans '{LOG_FILE}'.")
