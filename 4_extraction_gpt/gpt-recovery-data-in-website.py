import json
import requests
import time
import os
from openai import OpenAI
from bs4 import BeautifulSoup

# üìÇ Chemins des fichiers mis √† jour
INPUT_FILE = "3_filtrage_congres/liste_congr√®s_reformat.json"
OUTPUT_FILE = "4_extraction_gpt/congres_enrichis.json"
LOG_FILE = "4_extraction_gpt/gpt_extraction_errors.log"

# Activer ou d√©sactiver le mode debug
DEBUG_MODE = True

# V√©rifier si le fichier d'entr√©e existe
if not os.path.exists(INPUT_FILE):
    print(f"‚ùå Erreur : Le fichier {INPUT_FILE} n'existe pas.")
    exit(1)

# Lecture de la cl√© API et de l'ID d'organisation
with open('key.txt', 'r') as file:
    secret_key = file.read().strip()

# Initialisation du client OpenAI
client = OpenAI(api_key=secret_key)

# Charger la liste des congr√®s
with open(INPUT_FILE, "r", encoding="utf-8") as file:
    congres_data = json.load(file)

if not congres_data:
    print("‚ùå Aucun congr√®s trouv√© dans le fichier d'entr√©e.")
    exit(1)

# V√©rifier si un fichier de sauvegarde existe d√©j√† (pour reprendre en cas d'arr√™t)
if os.path.exists(OUTPUT_FILE):
    with open(OUTPUT_FILE, "r", encoding="utf-8") as file:
        resultats = json.load(file)
    print(f"üîÑ Reprise du traitement : {len(resultats)} congr√®s d√©j√† trait√©s.")
else:
    resultats = []

traites = {item["lien"] for item in resultats}  # Liens d√©j√† trait√©s

# Fonction pour r√©cup√©rer et nettoyer le contenu d'une page web
def recuperer_contenu(url):
    try:
        response = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")
        return soup.get_text()
    except requests.RequestException as e:
        with open(LOG_FILE, "a", encoding="utf-8") as log_file:
            log_file.write(f"‚ùå Erreur acc√®s {url}: {e}\n")
        print(f"‚ùå Erreur acc√®s {url}: {e}")
        return None

# Fonction pour d√©couper un texte en blocs de 5000 caract√®res
def decouper_texte(texte, taille_bloc=5000):
    return [texte[i:i+taille_bloc] for i in range(0, len(texte), taille_bloc)]

# Fonction pour demander √† GPT d'extraire les infos sous format JSON
def extraire_informations(text, url):
    blocs = decouper_texte(text)
    extraction_finale = {
        "contacts": [],
        "personnes": [],
        "lieux": [],
        "dates": [],
        "r√©sum√©": ""
    }

    for i, bloc in enumerate(blocs):
        print(f"‚Üí Envoi du bloc {i+1}/{len(blocs)} pour {url}...")

        prompt = f"""
        Voici un extrait d'une page web issue de {url}. Analyse-le et retourne **uniquement** un JSON **valide** en **fran√ßais**, contenant :

        {{
            "contacts": [
                {{"type": "email", "valeur": "email@example.com", "propri√©taire": "Dr Jean Dupont"}},
                {{"type": "t√©l√©phone", "valeur": "+33 1 23 45 67 89", "propri√©taire": "Universit√© de Paris"}}
            ],
            "personnes": [
                {{"nom": "Dr Jean Dupont", "titre": "Professeur en ophtalmologie", "affiliation": "Universit√© de Paris",
                  "contact": "jean.dupont@example.com", "t√©l√©phone": "+33 1 23 45 67 89", "publications": ["Titre publication 1", "Titre publication 2"]}}
            ],
            "lieux": ["Paris, Centre des Congr√®s"],
            "dates": [
                {{"date": "10-12 mars 2025", "√©v√©nement": "Conf√©rence sur l'imagerie m√©dicale", "personnes_associees": ["Dr Jean Dupont", "Professeur Martin"]}}
            ],
            "r√©sum√©": "Conf√©rence internationale sur les derni√®res avanc√©es en ophtalmologie."
        }}

        Extrait :
        {bloc}
        """

        # Syst√®me de gestion du rate limit avec r√©essai
        attente = 2
        for tentative in range(5):  # Essaye 5 fois max
            try:
                response = client.chat.completions.create(
                    model="gpt-4",
                    messages=[{"role": "user", "content": prompt}]
                )
                contenu_gpt = response.choices[0].message.content.strip()

                if DEBUG_MODE:
                    print(f"üßê R√©ponse brute de GPT (Bloc {i+1}):\n{contenu_gpt}")

                if contenu_gpt.startswith("{") and contenu_gpt.endswith("}"):
                    resultat = json.loads(contenu_gpt)
                else:
                    raise ValueError("La r√©ponse de GPT n'est pas un JSON valide.")

                extraction_finale["contacts"].extend(resultat.get("contacts", []))
                extraction_finale["personnes"].extend(resultat.get("personnes", []))
                extraction_finale["lieux"].extend(resultat.get("lieux", []))
                extraction_finale["dates"].extend(resultat.get("dates", []))
                extraction_finale["r√©sum√©"] += " " + resultat.get("r√©sum√©", "")

                break  # Sortir de la boucle en cas de succ√®s

            except Exception as e:
                print(f"‚ö†Ô∏è Erreur avec GPT (tentative {tentative+1}/5) : {e}")
                time.sleep(attente)
                attente *= 2  # Double le temps d'attente √† chaque √©chec

    return extraction_finale

# Processus principal
total_congres = len(congres_data)
for index, item in enumerate(congres_data):
    if isinstance(item, str):
        url = item
    elif isinstance(item, dict) and "lien" in item:
        url = item["lien"]
    else:
        print(f"‚ö†Ô∏è Format invalide d√©tect√© : {item}")
        continue

    if url in traites:
        print(f"‚è© {url} d√©j√† trait√©, passage au suivant...")
        continue

    print(f"\nüîç [{index+1}/{total_congres}] Traitement de {url}...")
    contenu = recuperer_contenu(url)

    if contenu:
        infos = extraire_informations(contenu, url)
        resultats.append({"lien": url, "extraction": infos})
        traites.add(url)

    # Sauvegarde toutes les 5 requ√™tes
    if len(resultats) % 5 == 0:
        with open(OUTPUT_FILE, "w", encoding="utf-8") as output_file:
            json.dump(resultats, output_file, indent=4, ensure_ascii=False)
        print("üíæ Sauvegarde interm√©diaire effectu√©e.")

# Sauvegarde finale
with open(OUTPUT_FILE, "w", encoding="utf-8") as output_file:
    json.dump(resultats, output_file, indent=4, ensure_ascii=False)

print(f"\n‚úÖ Extraction termin√©e. R√©sultats sauvegard√©s dans '{OUTPUT_FILE}'.")
